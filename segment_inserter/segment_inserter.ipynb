{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebd3131",
   "metadata": {},
   "source": [
    "基本的には、まず、kmlを読み込むことでどのsemanticでsegmentを生成するのを指定します。そして、生成したいsegmentの間隔を指定して、それに基づいて先に該当semanticにおいて、segmentを生成します。  \n",
    "ここで、各segmentの経度緯度が分かりました。その経度緯度を訓練セットとして最近傍法(k＝1)で分類モデルをトレニングします。\n",
    "そして、ゼンリンのjsonファイルから、データを読み込んで、ゼンリンの各ポイントをさっきトレニングした分類モデルを使って、各segmentに分配します。  \n",
    "\n",
    "あるsemanticにおいて、segmentの経度緯度の密度とゼンリンデータのポイントの密度が違うため、segmentに分配されていない部分があり、その部分をデータの欠損と見做し、欠損のある点の前後に欠損ではない点を使って加重平均の方法で欠損を埋めます。  \n",
    "そして、補正されたデータをSQLサーバーに挿入します。 \n",
    "\n",
    "---\n",
    "<font color=red>下記はアルゴリズムのフローチャートです。</font>  \n",
    "![img](img/chart.png)  \n",
    "  \n",
    "---\n",
    "<font color=red>下記は欠損補正部分のアルゴリズムの説明です。</font>  \\\n",
    "![img](img/加重平均の説明.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ef687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from os import path\n",
    "import json\n",
    "import time\n",
    "import pyodbc\n",
    "import webbrowser\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "import folium\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# ヒュベニの公式\n",
    "# https://butter-tiger.hatenablog.com/entry/2020/08/20/222650\n",
    "POLE_RADIUS = 6356752  # 極半径(短半径)\n",
    "EQUATOR_RADIUS = 6378137  # 赤道半径(長半径)\n",
    "E = 0.081819191042815790  # 離心率\n",
    "E2 = 0.006694380022900788  # 離心率の２乗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6dff2",
   "metadata": {},
   "source": [
    "<font size = 6 color =red>ここからは必要な関数を定義する↓</font>  \n",
    "  \n",
    "<font size = 3>この部分はJSONファイルに関する計算やファイルの処理</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33906b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_inserter_path = rf\"/segment_inserter/\"\n",
    "\n",
    "FOLDER_PATH_json = rf\"\\FILES\"  \n",
    "GIS_BOOL = 0  # 10M****標高と比較する場合は1，そうでない場合は0\n",
    "\n",
    "JSON_PATH = rf\"\\JSON\"  # 入力フォルダ\n",
    "JSON_inserted_PATH = rf\"\\JSON_inserted\"  # 入力済みフォルダ\n",
    "\n",
    "def distance(_lat1, _long1, _lat2, _long2):\n",
    "    lat1 = math.radians(_lat1)\n",
    "    long1 = math.radians(_long1)\n",
    "    lat2 = math.radians(_lat2)\n",
    "    long2 = math.radians(_long2)\n",
    "    m_lat = (lat1 + lat2) / 2  # 平均緯度\n",
    "    d_lat = abs(lat1 - lat2)  # 緯度差\n",
    "    d_lon = abs(long1 - long2)  # 経度差\n",
    "    W = math.sqrt(1 - E2 * math.pow(math.sin(m_lat), 2))\n",
    "    M = EQUATOR_RADIUS * (1 - E2) / math.pow(W, 3)  # 子午線曲率半径\n",
    "    N = EQUATOR_RADIUS / W  # 卯酉線曲率半径\n",
    "    # d = math.sqrt(math.pow(M*d_lat,2) + math.pow(N*d_lon*math.cos(m_lat),2) + math.pow(point_a.altitude-point_b.altitude,2))\n",
    "    d = math.sqrt(math.pow(M * d_lat, 2) + math.pow(N * d_lon * math.cos(m_lat), 2))\n",
    "    return d\n",
    "\n",
    "def search_filelist():\n",
    "    \"\"\"toinsert/ ディレクトリに含まれるファイルリストを返す関数.\n",
    "    Returns:\n",
    "        [string]: toinsert/ ディレクトリに含まれるファイルパスのリスト.\n",
    "    \"\"\"\n",
    "    p = Path(path.join(path.dirname(os.getcwd()), \".\" +\"/segment_inserter/\"+ FOLDER_PATH + JSON_PATH))\n",
    "\n",
    "    filenames = []\n",
    "    print(\"filelist:\")\n",
    "\n",
    "    for file in p.iterdir():\n",
    "        if file.is_dir():\n",
    "            continue\n",
    "\n",
    "        # pathstrings = str(file).split('/')\n",
    "        # filename = file.name\n",
    "        # print(pathstrings)\n",
    "        if re.match(\".+\" + r\".json\", file.name):\n",
    "            print(\"- \", file.name, \":JSON file\")\n",
    "            filenames.append(\"{}\".format(file.name))\n",
    "            # filenames.append(FOLDER_PATH + JSON_PATH + '\\\\{}'.format(file.name))\n",
    "        else:\n",
    "            print(file.name, \":not JSON file:\")\n",
    "        # filenames.append(file.name)\n",
    "    print()\n",
    "    return filenames\n",
    "\n",
    "# SUB FUNC\n",
    "## UTF8形式のjsonファイルを読み込みdictに格納\n",
    "def load_json_to_dict_UTF8(input_jsonpath):\n",
    "    fp = open(input_jsonpath, \"r\", encoding=\"utf-8_sig\")\n",
    "    json_load = json.load(fp)\n",
    "    return json_load\n",
    "\n",
    "# ここから10M****標高取得関連\n",
    "# 緯度、経度の組を入力とし、****の10M****標高を参照して標高を返す関数\n",
    "# ゼンリンと国土地理院の比較用に用意\n",
    "def altDao(_lat, _long):\n",
    "    connect = pyodbc.connect(\n",
    "        \"DRIVER={SQL Server};SERVER=\"\n",
    "        + SERVER\n",
    "        + \";UID=\"\n",
    "        + uid\n",
    "        + \";PWD=\"\n",
    "        + pwd\n",
    "        + \";DATABASE=\"\n",
    "        + DATABASE\n",
    "        + \";Trusted_Connection=\"\n",
    "        + trusted_connection\n",
    "        + \";\"\n",
    "    )\n",
    "    altitude = -99\n",
    "    sql = (\n",
    "        \"SELECT ALTITUDE FROM [****].[dbo].[ALTITUDE_10M_****] WHERE LOWER_LATITUDE <= \"\n",
    "        + _lat\n",
    "        + \"AND UPPER_LATITUDE > \"\n",
    "        + _lat\n",
    "        + \"AND LOWER_LONGITUDE <= \"\n",
    "        + _long\n",
    "        + \"AND UPPER_LONGITUDE > \"\n",
    "        + _long\n",
    "    )\n",
    "    rows = select_execute(connect, sql)\n",
    "    if len(rows) > 0:\n",
    "        altitude = round(float(str(rows[0]).replace(\", )\", \"\").replace(\"(\", \"\")), 3)\n",
    "    # print(str(i) + ' -> ' + str(altitude))\n",
    "    connect.close()\n",
    "    # print('10M**** Altitude : ' + str(altitude))\n",
    "    return altitude\n",
    "\n",
    "\n",
    "def select_execute(con, sql):\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ここまで10M****標高取得関連\n",
    "\n",
    "# convert JSON to [Latitude, Longitude, ZENRIN_Elevation]\n",
    "\n",
    "def ADASListGenerator_to_df(filename):\n",
    "    json_semanticlinkID_name = filename.replace(\".json\", \"\")  # ここを変更\n",
    "    ## filepath\n",
    "    curdir_dirpath = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    json_zenrin_filepath = (\n",
    "        curdir_dirpath +\"/segment_inserter/\"+ FOLDER_PATH_json + JSON_PATH + \"\\\\\" + filename\n",
    "    )  # ECOLOG_CANdataレコード取得用sql\n",
    "    print(\"next_file：\", json_zenrin_filepath)\n",
    "\n",
    "    if GIS_BOOL == 1:\n",
    "        data_json = pd.DataFrame({'SemanticLink':[],\n",
    "                                  'Latitude':[],\n",
    "                                  'Longitude':[],\n",
    "                                  'ZENRIN_Elevation':[],\n",
    "                                  'adas_null':[],\n",
    "                                  'GIS':[]})\n",
    "    elif GIS_BOOL == 0:\n",
    "        data_json = pd.DataFrame({'SemanticLink':[],\n",
    "                                  'Latitude':[],\n",
    "                                  'Longitude':[],\n",
    "                                  'ZENRIN_Elevation':[],\n",
    "                                  'adas_null':[]})\n",
    "    else:\n",
    "        print(\"GIS_BOOLは0か1にしてください\")\n",
    "\n",
    "    ## データ抽出\n",
    "    zenrin_response_json_list = []\n",
    "    zenrin_response_json_list = load_json_to_dict_UTF8(json_zenrin_filepath)  # jsonの大外がリスト構造\n",
    "    global adas_null \n",
    "    \n",
    "\n",
    "    for zenrin_response_json in zenrin_response_json_list:\n",
    "        for path in zenrin_response_json[\"result\"][\"path\"]:  # マッチした全リンク情報の取り出し\n",
    "\n",
    "            if \"adas\" in path[\"matchLink\"] and \"roadelevation\" in str(path[\"matchLink\"]):\n",
    "                adas_null = 0\n",
    "\n",
    "                if \"\\\"adas\\\": null\" in path[\"matchLink\"] and \"roadelevation\" in str(path[\"matchLink\"]):\n",
    "                    adas_null = 1\n",
    "                link_number = 0  # ゼンリンリンクごとに振られる番号\n",
    "                for adasPoint in path[\"matchLink\"][\"adas\"][\"roadelevation\"]:  # adas由来の標高抽出\n",
    "                    \n",
    "                    # print(\"DRIVING DIRECTION = xxx\") # DB入力の際には必要\n",
    "                    if GIS_BOOL == 1:\n",
    "                        row = {\n",
    "                            \"ZenrinLinkId\": path[\"matchLink\"][\"code\"],\n",
    "                            \"Latitude\": adasPoint[\"lat\"],\n",
    "                            \"Longitude\": adasPoint[\"lon\"],\n",
    "                            \"Elevation\": adasPoint[\"elevation\"] / 1000.0,\n",
    "                        }\n",
    "                        GIS_elevation = altDao(str(adasPoint[\"lat\"]), str(adasPoint[\"lon\"]))\n",
    "\n",
    "                        data_json.loc[len(data_json)+1] = [json_semanticlinkID_name, adasPoint[\"lat\"], adasPoint[\"lon\"], adasPoint[\"elevation\"],adas_null, GIS_elevation]\n",
    "                        \n",
    "\n",
    "                    elif GIS_BOOL == 0:\n",
    "                    \n",
    "                        row = {\n",
    "                            \"ZenrinLinkId\": path[\"matchLink\"][\"code\"],\n",
    "                            \"Latitude\": adasPoint[\"lat\"],\n",
    "                            \"Longitude\": adasPoint[\"lon\"],\n",
    "                        }\n",
    "                        data_json.loc[len(data_json)+1] = [json_semanticlinkID_name, adasPoint[\"lat\"], adasPoint[\"lon\"], adasPoint[\"elevation\"], adas_null]\n",
    "\n",
    "\n",
    "                    link_number = link_number + 1\n",
    "        data_json.reset_index(drop = True,inplace = True)\n",
    "        return data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9cc78",
   "metadata": {},
   "source": [
    "<font size = 3>この部分はKMLファイルに関する計算やファイルの処理</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ccdd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_jst = datetime.datetime.now()\n",
    "now_jst_result = str(now_jst.strftime('%Y%m%d-%H%M%S'))\n",
    "#FOLDER_PATH = \"\\\\{}\".format(now_jst_result)  # 作業フォルダ\n",
    "FOLDER_PATH = \"\"  # 作業フォルダ\n",
    "KML_PATH = rf\"\\KML_DrivingRoute\"  # 入力フォルダ\n",
    "# 入力済みフォルダ\n",
    "KML_inserted_PATH_10m = rf\"\\KML_DrivingRoute_inserted\\10m\"  \n",
    "KML_inserted_PATH_50m = rf\"\\KML_DrivingRoute_inserted\\50m\"  \n",
    "KML_inserted_PATH_100m = rf\"\\KML_DrivingRoute_inserted\\100m\"  \n",
    "\n",
    "KML_inserted_PATH_ELEVATION_null = rf\"\\KML_DrivingRoute_inserted\\ELEVATION_null\"  \n",
    "\n",
    "\n",
    "CSV_PATH = rf\"\\CSV_DrivingLog\"  # 出力フォルダ\n",
    "CSV_FOLIUM_PATH = r\"\\CSV_FoliumMap\"  # CSV の folium マップ出力フォルダ\n",
    "#ACCESS_NUMBER = 5  # いつもNAVI APIアクセス数（1アクセスにつき100入力まで）\n",
    "#CONSTANT_SPEED = 100 # 定速速度はファイル名から自動で判別するようにする（この値はデフォルト値）\n",
    "\n",
    "# ヒュベニの公式\n",
    "# https://butter-tiger.hatenablog.com/entry/2020/08/20/222650\n",
    "POLE_RADIUS = 6356752  # 極半径(短半径)\n",
    "EQUATOR_RADIUS = 6378137  # 赤道半径(長半径)\n",
    "E = 0.081819191042815790  # 離心率\n",
    "E2 = 0.006694380022900788  # 離心率の２乗\n",
    "\n",
    "def distance(_lat1, _long1, _lat2, _long2):\n",
    "    lat1 = math.radians(_lat1)\n",
    "    long1 = math.radians(_long1)\n",
    "    lat2 = math.radians(_lat2)\n",
    "    long2 = math.radians(_long2)\n",
    "    m_lat = (lat1 + lat2) / 2  # 平均緯度\n",
    "    d_lat = abs(lat1 - lat2)  # 緯度差\n",
    "    d_lon = abs(long1 - long2)  # 経度差\n",
    "    W = math.sqrt(1 - E2 * math.pow(math.sin(m_lat), 2))\n",
    "    M = EQUATOR_RADIUS * (1 - E2) / math.pow(W, 3)  # 子午線曲率半径\n",
    "    N = EQUATOR_RADIUS / W  # 卯酉線曲率半径\n",
    "    # d = math.sqrt(math.pow(M*d_lat,2) + math.pow(N*d_lon*math.cos(m_lat),2) + math.pow(point_a.altitude-point_b.altitude,2))\n",
    "    d = math.sqrt(math.pow(M * d_lat, 2) + math.pow(N * d_lon * math.cos(m_lat), 2))\n",
    "    return d\n",
    "\n",
    "\n",
    "def search_filelist_kml():\n",
    "    \n",
    "    p = Path(rf\"{os.getcwd()}{KML_PATH}\")\n",
    "    print(p)\n",
    "\n",
    "    filenames = []\n",
    "\n",
    "    for file in p.iterdir():\n",
    "        if file.is_dir():\n",
    "            continue\n",
    "\n",
    "        # pathstrings = str(file).split('/')\n",
    "        # filename = file.name\n",
    "        # print(pathstrings)\n",
    "        if re.match(\".+\" + r\".kml\", file.name):\n",
    "            print(file.name, \":KML file\")\n",
    "            filenames.append(\n",
    "                rf\"{os.getcwd()}{KML_PATH}\\{file.name}\"\n",
    "            )\n",
    "        else:\n",
    "            print(file.name, \":not KML file:\")\n",
    "        # filenames.append(file.name)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# KMLファイルの中から，必要な座標列のみを抽出し，リスト形式で返すメソッド\n",
    "def readLinksKML(filename):\n",
    "    lineSirungTab1 = r\"<LineString>\"\n",
    "    lineSirungTab2 = r\"</LineString>\"\n",
    "    isLineString = 0\n",
    "    f = open(filename, \"r\", encoding=\"UTF-8\")\n",
    "    datalist = f.readlines()\n",
    "    GPSlist = [[\"Latitude\", \"Longitude\"]]\n",
    "    for data in datalist:\n",
    "        if isLineString == 1:\n",
    "            if lineSirungTab2 in data:\n",
    "                # print('ここまでが対象データ')\n",
    "                isLineString = 0\n",
    "            else:\n",
    "                target_data = data.replace(\" \", \"\").replace(r\",0\", \"\")\n",
    "                if re.match(r\"[0-9]+.?[0-9]*,[0-9]+.?[0-9]*\", target_data):\n",
    "                    target_data2 = re.split(\",\", target_data)\n",
    "                    appendList = [float(target_data2[1]), float(target_data2[0])]\n",
    "                    GPSlist.append(appendList)\n",
    "                    # print(appendList)\n",
    "        elif isLineString == 0:\n",
    "            if lineSirungTab1 in data:\n",
    "                # print('ここからが対象データ')\n",
    "                isLineString = 1\n",
    "        else:\n",
    "            print(\"よきせぬれーがい\")\n",
    "    GPSlist.remove([\"Latitude\", \"Longitude\"])\n",
    "    return GPSlist\n",
    "\n",
    "\n",
    "def calcDistance(list_GPS):\n",
    "    list_lat_long_dist = [\n",
    "        [\"Latitude1\", \"Longitude1\", \"Latitude2\", \"Longitude2\", \"Distance\"]\n",
    "    ]\n",
    "    before_lat = 0\n",
    "    before_long = 0\n",
    "    dist_sum = 0\n",
    "    for row in list_GPS:\n",
    "        if before_lat > 0:\n",
    "            dist = distance(before_lat, before_long, row[0], row[1])\n",
    "            appendList = [before_lat, before_long, row[0], row[1], dist]\n",
    "            list_lat_long_dist.append(appendList)\n",
    "            dist_sum = dist_sum + dist\n",
    "            # print(appendList)\n",
    "        before_lat = row[0]\n",
    "        before_long = row[1]\n",
    "    list_lat_long_dist.remove(\n",
    "        [\"Latitude1\", \"Longitude1\", \"Latitude2\", \"Longitude2\", \"Distance\"]\n",
    "    )\n",
    "    return list_lat_long_dist, dist_sum\n",
    "\n",
    "\n",
    "def normalizedCoordinatesGenerator(list_lat_long_dist, normalized_dist):\n",
    "    normalizedList = [[\"Latitude\", \"Longitude\"]]\n",
    "    rest = normalized_dist\n",
    "    _lat = list_lat_long_dist[0][0]\n",
    "    _long = list_lat_long_dist[0][1]\n",
    "    appendList = [_lat, _long]\n",
    "    normalizedList.append(appendList)\n",
    "    coordinatesNumber = 1\n",
    "\n",
    "    for row in list_lat_long_dist:\n",
    "        row_loop = 0\n",
    "        if rest > row[4]:\n",
    "            rest = rest - row[4]\n",
    "            # print('rest:', str(rest))\n",
    "        else:\n",
    "            while rest + normalized_dist * row_loop < row[4]:\n",
    "                _lat = (row[2] - row[0]) * (rest + normalized_dist * row_loop) / row[\n",
    "                    4\n",
    "                ] + row[0]\n",
    "                _long = (row[3] - row[1]) * (rest + normalized_dist * row_loop) / row[\n",
    "                    4\n",
    "                ] + row[1]\n",
    "                appendList = [_lat, _long]\n",
    "                normalizedList.append(appendList)\n",
    "                coordinatesNumber = coordinatesNumber + 1\n",
    "                row_loop = row_loop + 1\n",
    "                # print('rest:', str(rest))\n",
    "            rest = rest - row[4] + normalized_dist * row_loop\n",
    "    \n",
    "    return normalizedList, coordinatesNumber\n",
    "\n",
    "coordinatesNumber = 0.00\n",
    "def GPSListGenerator_segment(filename):\n",
    "    print(\"try generate {}.\".format(filename))\n",
    "    list_GPS = readLinksKML(filename)  # KMLファイルの中から，必要な座標列のみを抽出し，リスト形式で返す\n",
    "    # print(list_GPS)    #(lat, long)\n",
    "    list_lat_long_dist, dist_sum = calcDistance(list_GPS)\n",
    "    CONSTANCE_SPEED = distance_between_segments*3.6\n",
    "    print(' ')\n",
    "    NORMALIZED_DISTANCE = CONSTANCE_SPEED * 1000 / 3600\n",
    "\n",
    "    normalized_dist = NORMALIZED_DISTANCE\n",
    "\n",
    "    list_normalized_lat_long, coordinatesNumber = normalizedCoordinatesGenerator(\n",
    "        list_lat_long_dist, normalized_dist\n",
    "    )\n",
    "    \n",
    "    return list_normalized_lat_long\n",
    "\n",
    "#上のGPSListGeneratorのoutputをcsvファイルとして出力\n",
    "def output_lat_long(filename, list_normalized_lat_long):\n",
    "    filename_csv = filename.replace(KML_PATH, CSV_PATH).replace(r'.kml', r'.csv')\n",
    "    print(\"output file:\", filename_csv, \"    座標数:\", str(coordinatesNumber), \"\\n\")\n",
    "    # 2次元配列→CSV変換:https://rikei-danshi.work/entry/python-2darray-csv\n",
    "    with open(filename_csv, \"w\") as file:\n",
    "        writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "        writer.writerows(list_normalized_lat_long)\n",
    "\n",
    "\n",
    "def move_file(filename):\n",
    "    filename_kml_inserted = filename.replace(KML_PATH, KML_inserted_PATH)\n",
    "    shutil.move(filename, filename_kml_inserted)\n",
    "\n",
    "\n",
    "def display_folium_map_from_csv(data,path):\n",
    "    output_folium_file_path = path\n",
    "    df_output_points = data.iloc[:,0:2]\n",
    "    folium_map = folium.Map(\n",
    "        location=[\n",
    "            df_output_points.at[len(df_output_points) // 2, \"Latitude\"],\n",
    "            df_output_points.at[len(df_output_points) // 2, \"Longitude\"],\n",
    "        ],\n",
    "        zoom_start=13,\n",
    "    )\n",
    "    for index, point in df_output_points.iterrows():\n",
    "        folium.Marker(location=[point[\"Latitude\"], point[\"Longitude\"]]).add_to(\n",
    "            folium_map\n",
    "        )\n",
    "    folium_map.save(output_folium_file_path)\n",
    "    webbrowser.open(output_folium_file_path, new=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6a03b",
   "metadata": {},
   "source": [
    "<font size = 3>この部分はSEGMENTS_****のテーブル群のインサーター関数</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7af918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_name_SEGMENTS_****_10m():\n",
    "    return \"SEGMENTS_****_10M\"\n",
    "\n",
    "def table_name_SEGMENTS_****_50m():\n",
    "    return \"SEGMENTS_****_50M\"\n",
    "\n",
    "def table_name_SEGMENTS_****_100m():\n",
    "    return \"SEGMENTS_****_100M\"\n",
    "\n",
    "def column_list_SEGMENTS_****():\n",
    "    return \"(SEGMENT_ID,SEGMENT_LENGTH,START_LATITUDE,START_LONGITUDE,END_LATITUDE,END_LONGITUDE,START_ADAS_ELEVATION_MILLI_METER,END_ADAS_ELEVATION_MILLI_METER,SLOPE_ANGLE_THETA,COS_THETA,SIN_THETA,IS_ADAS_NULL,IS_GET_****_DATA)\"\n",
    "\n",
    "def insert_data_to_SEGMENTS_****_10M(data):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    for i in range(len(data)):\n",
    "        list1 = data.loc[i].to_list()\n",
    "        stmt = \"\"\"\n",
    "        INSERT INTO {} {} \n",
    "        VALUES {}\n",
    "        \"\"\".format(table_name_SEGMENTS_****_10m(),column_list_SEGMENTS_****(),tuple(list1))\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            cursor.commit()\n",
    "        except pyodbc.IntegrityError as err:\n",
    "                        # 主キー違反の場合には読み飛ばす\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "                            print(\"---\")\n",
    "                            print(e)\n",
    "                            print(\"HINT: トリップの最初のレコードだけエラーが出る様子.\")\n",
    "                            print(\"Excecuted SQL below.\")\n",
    "                            print(stmt)\n",
    "                            print(\"---\")\n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    \n",
    "def insert_data_to_SEGMENTS_****_50M(data):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    for i in range(len(data)):\n",
    "        list1 = data.loc[i].to_list()\n",
    "        stmt = \"\"\"\n",
    "        INSERT INTO {} {} \n",
    "        VALUES {}\n",
    "        \"\"\".format(table_name_SEGMENTS_****_50m(),column_list_SEGMENTS_****(),tuple(list1))\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            cursor.commit()\n",
    "        except pyodbc.IntegrityError as err:\n",
    "                        # 主キー違反の場合には読み飛ばす\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "                            print(\"---\")\n",
    "                            print(e)\n",
    "                            print(\"HINT: トリップの最初のレコードだけエラーが出る様子.\")\n",
    "                            print(\"Excecuted SQL below.\")\n",
    "                            print(stmt)\n",
    "                            print(\"---\")\n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    \n",
    "def insert_data_to_SEGMENTS_****_100M(data):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    for i in range(len(data)):\n",
    "        list1 = data.loc[i].to_list()\n",
    "        stmt = \"\"\"\n",
    "        INSERT INTO {} {} \n",
    "        VALUES {}\n",
    "        \"\"\".format(table_name_SEGMENTS_****_100m(),column_list_SEGMENTS_****(),tuple(list1))\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            cursor.commit()\n",
    "        except pyodbc.IntegrityError as err:\n",
    "                        # 主キー違反の場合には読み飛ばす\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "                            print(\"---\")\n",
    "                            print(e)\n",
    "                            print(\"HINT: トリップの最初のレコードだけエラーが出る様子.\")\n",
    "                            print(\"Excecuted SQL below.\")\n",
    "                            print(stmt)\n",
    "                            print(\"---\")\n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "#新しい間隔のセグメントの挿入関数はここで定義\n",
    "#下のSEMANTIC_LINKS_SEGMENTSのテーブル群のインサーター関数も忘れずに"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0eecc1",
   "metadata": {},
   "source": [
    "<font size = 3>この部分はSEMANTIC_LINKS_SEGMENTSのテーブル群のインサーター関数</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f885a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_name_SEMANTIC_LINKS_SEGMENTS_10m():\n",
    "    return \"SEMANTIC_LINKS_SEGMENTS_10M\"\n",
    "\n",
    "def table_name_SEMANTIC_LINKS_SEGMENTS_50m():\n",
    "    return \"SEMANTIC_LINKS_SEGMENTS_50M\"\n",
    "\n",
    "def table_name_SEMANTIC_LINKS_SEGMENTS_100m():\n",
    "    return \"SEMANTIC_LINKS_SEGMENTS_100M\"\n",
    "\n",
    "def column_list_SEMANTIC_LINKS_SEGMENTS():\n",
    "    return \"(SEMANTIC_LINK_ID,SEMANTIC_LINK_SEGMENT_ID,SEGMENT_ID)\"\n",
    "\n",
    "def insert_data_to_SEMANTIC_LINKS_SEGMENTS_10M(data):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    for i in range(len(data)):\n",
    "        list1 = data.loc[i].to_list()\n",
    "        stmt = \"\"\"\n",
    "        INSERT INTO {} {} \n",
    "        VALUES {}\n",
    "        \"\"\".format(table_name_SEMANTIC_LINKS_SEGMENTS_10m(),column_list_SEMANTIC_LINKS_SEGMENTS(),tuple(list1))\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            cursor.commit()\n",
    "        except pyodbc.IntegrityError as err:\n",
    "                        # 主キー違反の場合には読み飛ばす\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "                            print(\"---\")\n",
    "                            print(e)\n",
    "                            print(\"HINT: トリップの最初のレコードだけエラーが出る様子.\")\n",
    "                            print(\"Excecuted SQL below.\")\n",
    "                            print(stmt)\n",
    "                            print(\"---\")\n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    \n",
    "def insert_data_to_SEMANTIC_LINKS_SEGMENTS_50M(data):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    for i in range(len(data)):\n",
    "        list1 = data.loc[i].to_list()\n",
    "        stmt = \"\"\"\n",
    "        INSERT INTO {} {} \n",
    "        VALUES {}\n",
    "        \"\"\".format(table_name_SEMANTIC_LINKS_SEGMENTS_50m(),column_list_SEMANTIC_LINKS_SEGMENTS(),tuple(list1))\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            cursor.commit()\n",
    "        except pyodbc.IntegrityError as err:\n",
    "                        # 主キー違反の場合には読み飛ばす\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "                            print(\"---\")\n",
    "                            print(e)\n",
    "                            print(\"HINT: トリップの最初のレコードだけエラーが出る様子.\")\n",
    "                            print(\"Excecuted SQL below.\")\n",
    "                            print(stmt)\n",
    "                            print(\"---\")\n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    \n",
    "def insert_data_to_SEMANTIC_LINKS_SEGMENTS_100M(data):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    for i in range(len(data)):\n",
    "        list1 = data.loc[i].to_list()\n",
    "        stmt = \"\"\"\n",
    "        INSERT INTO {} {} \n",
    "        VALUES {}\n",
    "        \"\"\".format(table_name_SEMANTIC_LINKS_SEGMENTS_100m(),column_list_SEMANTIC_LINKS_SEGMENTS(),tuple(list1))\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "            cursor.commit()\n",
    "        except pyodbc.IntegrityError as err:\n",
    "                        # 主キー違反の場合には読み飛ばす\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "                            print(\"---\")\n",
    "                            print(e)\n",
    "                            print(\"HINT: トリップの最初のレコードだけエラーが出る様子.\")\n",
    "                            print(\"Excecuted SQL below.\")\n",
    "                            print(stmt)\n",
    "                            print(\"---\")\n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "#新しい間隔のセグメントの挿入関数はここで定義\n",
    "#下のSEMANTIC_LINKS_SEGMENTSのテーブル群とSEGMENTS_****のテーブル群から、SEGMENT_idを取得する関数も忘れずに"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d21fc0",
   "metadata": {},
   "source": [
    "<font size = 3>この部分はSEMANTIC_LINKS_SEGMENTSのテーブル群とSEGMENTS_****のテーブル群から、</font>  \n",
    "<font size = 3>SEGMENT_idを取得する関数</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f700a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_id_SEGMENTS_****_10M():\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    cursor.execute( \"select max(SEGMENT_ID) from SEGMENTS_****_10M\") \n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    return rows\n",
    "\n",
    "def get_segment_id_SEGMENTS_****_50M():\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    cursor.execute( \"select max(SEGMENT_ID) from SEGMENTS_****_50M\") \n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    return rows\n",
    "\n",
    "def get_segment_id_SEGMENTS_****_100M():\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    cursor.execute( \"select max(SEGMENT_ID) from SEGMENTS_****_100M\") \n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    return rows\n",
    "#新しい間隔のセグメントの挿入関数はここで定義\n",
    "\n",
    "def get_semantic_id_from_tables(semantic_id,distance_between_segments):\n",
    "    driver='{SQL Server}'\n",
    "    server = '****'\n",
    "    database = '****'\n",
    "    trusted_connection='yes'\n",
    "    connect= pyodbc.connect('DRIVER='+driver+';SERVER='+server+';DATABASE='+database+';PORT=1433;Trusted_Connection='+trusted_connection+';')\n",
    "    cursor = connect.cursor()\n",
    "    table = \"SEMANTIC_LINKS_SEGMENTS_\" + str(distance_between_segments) + \"M\"\n",
    "    sql = \"select count(SEMANTIC_LINK_ID) from \" +table+ \" where SEMANTIC_LINK_ID = \" + str(semantic_id)\n",
    "    cursor.execute(sql) \n",
    "    rows = cursor.fetchall()\n",
    "    rows = int(list(rows[0])[0])\n",
    "    \n",
    "    cursor.close()\n",
    "    connect.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ea091",
   "metadata": {},
   "source": [
    "<font size = 6 color =red> ここからは実行↓ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    segment_id = 1\n",
    "    print('セグメントの間隔を指定してください(m),範囲は10,50,100から一つです：')\n",
    "    print('-------')\n",
    "    distance_between_segments = input()\n",
    "    distance_between_segments = int(distance_between_segments)\n",
    "    for filename in search_filelist_kml():\n",
    "        SEMANTIC_LINK_ID = int(filename.split('.')[-2].split('\\\\')[-1])\n",
    "        if get_semantic_id_from_tables(SEMANTIC_LINK_ID,distance_between_segments) != 0:\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'のデータはすでにSQLサーバーに存在しているので')\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対する挿入を停止する')\n",
    "            print('')\n",
    "            if distance_between_segments == 10:\n",
    "                KML_inserted_PATH = KML_inserted_PATH_10m\n",
    "            elif distance_between_segments == 50:\n",
    "                KML_inserted_PATH = KML_inserted_PATH_50m\n",
    "            elif distance_between_segments == 100:\n",
    "                KML_inserted_PATH = KML_inserted_PATH_100m\n",
    "            print(filename,'を',KML_inserted_PATH_100m,'に移動する')\n",
    "                \n",
    "            move_file(filename)\n",
    "            print('-------')\n",
    "            continue\n",
    "        \n",
    "        #get segment_id from tables and check if it is >1 and  check if distance_between_segments is correct\n",
    "        if distance_between_segments == 10:\n",
    "            if list(get_segment_id_SEGMENTS_****_10M()[0]) == [None]:\n",
    "                segment_id = 1\n",
    "            elif int(list(get_segment_id_SEGMENTS_****_10M()[0])[0]) >= 1 :\n",
    "                segment_id = int(list(get_segment_id_SEGMENTS_****_10M()[0])[0]) + 1\n",
    "        elif distance_between_segments == 50:\n",
    "            if list(get_segment_id_SEGMENTS_****_50M()[0]) == [None]:\n",
    "                segment_id = 1\n",
    "            elif int(list(get_segment_id_SEGMENTS_****_50M()[0])[0]) >= 1 :\n",
    "                segment_id = int(list(get_segment_id_SEGMENTS_****_50M()[0])[0]) + 1\n",
    "        elif distance_between_segments == 100:\n",
    "            if list(get_segment_id_SEGMENTS_****_100M()[0]) == [None]:\n",
    "                segment_id = 1\n",
    "            elif int(list(get_segment_id_SEGMENTS_****_100M()[0])[0]) >= 1 :\n",
    "                segment_id = int(list(get_segment_id_SEGMENTS_****_100M()[0])[0]) + 1\n",
    "        else:\n",
    "            print('セグメント間隔の指定に問題があります、必ず範囲内に指定してください')\n",
    "            break \n",
    "        #read kml and convert it to a DataFrame\n",
    "        #segmentを生成する\n",
    "        df_segment = pd.DataFrame(GPSListGenerator_segment(filename)[1:],columns =GPSListGenerator_segment(filename)[0])\n",
    "        segment_id_series = list(range(segment_id,segment_id+len(df_segment)))\n",
    "        #segment_id = int(segment_id_series[-1])+1\n",
    "        df_segment['segment_Coordinate_id'] = segment_id_series\n",
    "\n",
    "        #To match link_id of roads, a Knn classifier is trained with datas extracted from the df_segment\n",
    "        train_data = df_segment.iloc[:,0:2]\n",
    "        train_label = df_segment.iloc[:,-1]\n",
    "        Classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "        Classifier.fit(train_data,train_label)\n",
    "        #Use trained classifier to match roads\n",
    "        #you can trust this classifier cause of I set n_neighbors to 1,and it must overfitting\n",
    "        #so its accuracy will be 100%\n",
    "        if isinstance(SEMANTIC_LINK_ID, int):\n",
    "            json_file_name = str(SEMANTIC_LINK_ID) + '.json'\n",
    "            df_json = ADASListGenerator_to_df(json_file_name)\n",
    "            #print(df_json)\n",
    "        else:\n",
    "            print('kmlファイル名をsemantic_id（整数）にしてください')\n",
    "            print('例：455.kml')\n",
    "            break\n",
    "        \n",
    "        #該当セマンティックのjsonに標高データが存在しない場合：\n",
    "        if len(df_json) <1:\n",
    "            print('ゼンリンデータ：',json_file_name,' には標高データが存在していないため、')\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対する挿入を停止する')\n",
    "            print('')\n",
    "            print(filename,'を\\KML_DrivingRoute_inserted\\ELEVATION_nullに移動する')\n",
    "            \n",
    "            KML_inserted_PATH = KML_inserted_PATH_ELEVATION_null\n",
    "            move_file(filename)\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        Classifier_result_for_json = Classifier.predict(df_json.iloc[:,1:3])\n",
    "        #The following DataFrame was created to aggregate the training results and to organize the data\n",
    "        Data_for_inserting_SEGMENTS_**** = pd.DataFrame({'SEGMENT_ID':[],\n",
    "                                                        'SEGMENT_LENGTH':[],\n",
    "                                                        'START_LATITUDED':[],\n",
    "                                                        'START_LONGITUDE':[],\n",
    "                                                        'END_LATITUDE':[],\n",
    "                                                        'END_LONGITUDE':[],\n",
    "                                                        'START_ADAS_ELEVATION_MILLI_METER':[],\n",
    "                                                        'END_ADAS_ELEVATION_MILLI_METER':[],\n",
    "                                                        'SLOPE_ANGLE_THETA':[],\n",
    "                                                        'COS_THETA':[],\n",
    "                                                        'SIN_THETA':[],\n",
    "                                                        'IS_ADAS_NULL':[],\n",
    "                                                        'IS_GET_****_DATA':[]\n",
    "                                                        })\n",
    "        Data_for_inserting_SEGMENTS_****.SEGMENT_ID = df_segment.segment_Coordinate_id.iloc[0:-1]\n",
    "        Data_for_inserting_SEGMENTS_****.START_LATITUDED = df_segment.Latitude.iloc[0:-1]\n",
    "        Data_for_inserting_SEGMENTS_****.START_LONGITUDE = df_segment.Longitude.iloc[0:-1]\n",
    "        Data_for_inserting_SEGMENTS_****.END_LATITUDE = df_segment.Latitude.iloc[1:].reset_index(drop = True)\n",
    "        Data_for_inserting_SEGMENTS_****.END_LONGITUDE = df_segment.Longitude.iloc[1:].reset_index(drop = True)\n",
    "        \n",
    " \n",
    "        Data_for_inserting_SEMANTIC_LINKS_SEGMENTS = pd.DataFrame({'SEMANTIC_LINK_ID':[],\n",
    "                                                                   'SEMANTIC_LINK_SEGMENT_ID':[],\n",
    "                                                                   'SEGMENT_ID':[]\n",
    "                                                                   })\n",
    "        Data_for_inserting_SEMANTIC_LINKS_SEGMENTS.SEGMENT_ID = Data_for_inserting_SEGMENTS_****.SEGMENT_ID\n",
    "        Data_for_inserting_SEMANTIC_LINKS_SEGMENTS.SEMANTIC_LINK_ID = SEMANTIC_LINK_ID\n",
    "        for i in range(len(Data_for_inserting_SEMANTIC_LINKS_SEGMENTS)):\n",
    "            SEMANTIC_LINK_SEGMENT_ID = int(str(SEMANTIC_LINK_ID)+str(distance_between_segments)+ str(Data_for_inserting_SEMANTIC_LINKS_SEGMENTS.SEGMENT_ID.iloc[i]))\n",
    "            Data_for_inserting_SEMANTIC_LINKS_SEGMENTS.SEMANTIC_LINK_SEGMENT_ID.iloc[i] = SEMANTIC_LINK_SEGMENT_ID\n",
    "\n",
    "        #insert Classified df_json to Data_for_inserting_SEGMENTS_****\n",
    "        for i in range(len(Classifier_result_for_json)):\n",
    "            index = Data_for_inserting_SEGMENTS_****.SEGMENT_ID.isin([Classifier_result_for_json[i]])\n",
    "            Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.iloc[index] = df_json.ZENRIN_Elevation.loc[i]\n",
    "            Data_for_inserting_SEGMENTS_****.IS_ADAS_NULL.iloc[index] = df_json.adas_null.loc[i]\n",
    "        \n",
    "        #ここからはClassifierできない部分に対して、加重平均などの手法で欠損を埋める\n",
    "        index_after = 0\n",
    "        index_before = 0\n",
    "        index_NOT_nan_list = list(Data_for_inserting_SEGMENTS_****[~Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.isna()].index)\n",
    "        index_nan_list = list(Data_for_inserting_SEGMENTS_****[Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.isna()].index)\n",
    "        IS_GET_****_DATA=[]\n",
    "        for i in range(len(Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.isna())):\n",
    "            IS_GET_****_DATA.append(int(~Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.isna()[i]))\n",
    "        Data_for_inserting_SEGMENTS_****['IS_GET_****_DATA'] = IS_GET_****_DATA\n",
    "\n",
    "        before = 0\n",
    "        after = 0\n",
    "        if str(Data_for_inserting_SEGMENTS_****.iloc[0]['START_ADAS_ELEVATION_MILLI_METER']) == 'nan':\n",
    "            if Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.isnull().sum() == len(Data_for_inserting_SEGMENTS_****):\n",
    "                print('ゼンリンデータ：',json_file_name,' には標高データが存在していないため、')\n",
    "                print('セマンティック',SEMANTIC_LINK_ID,'に対する挿入を停止する')\n",
    "                continue\n",
    "            else:\n",
    "                Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[0] = Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[index_NOT_nan_list[0]]\n",
    "        if str(Data_for_inserting_SEGMENTS_****.iloc[-1]['START_ADAS_ELEVATION_MILLI_METER']) == 'nan':\n",
    "            Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[-1] = Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[index_NOT_nan_list[-1]]\n",
    "        #ここからは加重平均\n",
    "        while Data_for_inserting_SEGMENTS_****.START_ADAS_ELEVATION_MILLI_METER.isnull().sum() !=0:\n",
    "            for i in range(len(Data_for_inserting_SEGMENTS_****)-1):\n",
    "                if str(Data_for_inserting_SEGMENTS_****.iloc[i]['START_ADAS_ELEVATION_MILLI_METER'])== 'nan':\n",
    "                    before = Data_for_inserting_SEGMENTS_****.iloc[i-1]['START_ADAS_ELEVATION_MILLI_METER']\n",
    "                    index_before = i-1\n",
    "                    #print('index_before:',index_before,' ','before:',before)\n",
    "                    for j in range(i,len(Data_for_inserting_SEGMENTS_****)):\n",
    "                        if str(Data_for_inserting_SEGMENTS_****.iloc[j]['START_ADAS_ELEVATION_MILLI_METER'])!= 'nan':\n",
    "                            after = Data_for_inserting_SEGMENTS_****.iloc[j]['START_ADAS_ELEVATION_MILLI_METER']\n",
    "                            index_after = j\n",
    "                            #print('index_after:',index_after,' ','after:',after)\n",
    "                            break\n",
    "                    distance_before_to_after = distance(Data_for_inserting_SEGMENTS_****.iloc[index_before]['START_LATITUDED'], \n",
    "                                                        Data_for_inserting_SEGMENTS_****.iloc[index_before]['START_LONGITUDE'],\n",
    "                                                        Data_for_inserting_SEGMENTS_****.iloc[index_after]['START_LATITUDED'],\n",
    "                                                        Data_for_inserting_SEGMENTS_****.iloc[index_after]['START_LONGITUDE'])\n",
    "#                     if distance_before_to_after == 0:\n",
    "#                         print(index_before,Data_for_inserting_SEGMENTS_****.iloc[index_before]['START_LATITUDED'],Data_for_inserting_SEGMENTS_****.iloc[index_before]['START_LONGITUDE'])\n",
    "#                         print(index_after,Data_for_inserting_SEGMENTS_****.iloc[index_after]['START_LATITUDED'],Data_for_inserting_SEGMENTS_****.iloc[index_after]['START_LONGITUDE'])\n",
    "                    rate_before = distance(Data_for_inserting_SEGMENTS_****.iloc[index_before]['START_LATITUDED'], \n",
    "                                           Data_for_inserting_SEGMENTS_****.iloc[index_before]['START_LONGITUDE'],\n",
    "                                           Data_for_inserting_SEGMENTS_****.iloc[i]['START_LATITUDED'],\n",
    "                                           Data_for_inserting_SEGMENTS_****.iloc[i]['START_LONGITUDE'])/distance_before_to_after\n",
    "                    rate_after = distance(Data_for_inserting_SEGMENTS_****.iloc[i]['START_LATITUDED'], \n",
    "                                           Data_for_inserting_SEGMENTS_****.iloc[i]['START_LONGITUDE'],\n",
    "                                           Data_for_inserting_SEGMENTS_****.iloc[index_after]['START_LATITUDED'],\n",
    "                                           Data_for_inserting_SEGMENTS_****.iloc[index_after]['START_LONGITUDE'])/distance_before_to_after\n",
    "                    #print(before * (1-rate_before) + after*(1-rate_after))\n",
    "                    Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[i] = before * (1-rate_before) + after*(1-rate_after)\n",
    "                    #print('for this loop:',Data_for_inserting.iloc[i]['START_ADAS_ELEVATION_MILLI_METER'])\n",
    "                    break\n",
    "                    \n",
    "        #次のstartは前のend\n",
    "        Data_for_inserting_SEGMENTS_****['END_ADAS_ELEVATION_MILLI_METER'].iloc[1:] = Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[0:-1]\n",
    "        Data_for_inserting_SEGMENTS_****['END_ADAS_ELEVATION_MILLI_METER'].iloc[0] = Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[0]\n",
    "        #segment間隔を挿入\n",
    "        Data_for_inserting_SEGMENTS_****['SEGMENT_LENGTH'] = distance_between_segments\n",
    "        \n",
    "        #startとendの標高差を利用して勾配を算出\n",
    "        for i in range(len(Data_for_inserting_SEGMENTS_****)):\n",
    "            delta_ELEVATION = Data_for_inserting_SEGMENTS_****['END_ADAS_ELEVATION_MILLI_METER'].iloc[i] - Data_for_inserting_SEGMENTS_****['START_ADAS_ELEVATION_MILLI_METER'].iloc[i]\n",
    "            Data_for_inserting_SEGMENTS_****['SLOPE_ANGLE_THETA'].iloc[i] = math.atan(delta_ELEVATION/distance_between_segments)\n",
    "            Data_for_inserting_SEGMENTS_****['COS_THETA'].iloc[i] = math.cos(Data_for_inserting_SEGMENTS_****['SLOPE_ANGLE_THETA'].iloc[i])\n",
    "            Data_for_inserting_SEGMENTS_****['SIN_THETA'].iloc[i] = math.sin(Data_for_inserting_SEGMENTS_****['SLOPE_ANGLE_THETA'].iloc[i])\n",
    "            if str(Data_for_inserting_SEGMENTS_****.iloc[i]['IS_ADAS_NULL'])== 'nan':\n",
    "                Data_for_inserting_SEGMENTS_****['IS_ADAS_NULL'].iloc[i] = 0\n",
    "        Data_for_inserting_SEGMENTS_****['IS_ADAS_NULL'] = Data_for_inserting_SEGMENTS_****['IS_ADAS_NULL'].astype('int')\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #insert aggregated datas to SQL server\n",
    "        #dont forget to add new inserting method for new distance of segment\n",
    "        if distance_between_segments == 10:\n",
    "            KML_inserted_PATH = KML_inserted_PATH_10m\n",
    "            print('-------------------------------------')\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対して、SQLサーバーにデータを挿入しますฅ^•ω•^ฅ')\n",
    "            print('')\n",
    "            print('10m間隔のsegmentデータが挿入中です....')\n",
    "            print('')\n",
    "            print('今挿入中のテーブルはSEGMENTS_****_10M....')\n",
    "            print('')\n",
    "            insert_data_to_SEGMENTS_****_10M(Data_for_inserting_SEGMENTS_****)\n",
    "            print('SEGMENTS_****_10M挿入完了')\n",
    "            print('')\n",
    "            print('今挿入中のテーブルはSEMANTIC_LINKS_SEGMENTS_10M')\n",
    "            print('')\n",
    "            insert_data_to_SEMANTIC_LINKS_SEGMENTS_10M(Data_for_inserting_SEMANTIC_LINKS_SEGMENTS)\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対して、','全て挿入完了')\n",
    "            print('-------------------------------------')\n",
    "        elif distance_between_segments == 50:\n",
    "            KML_inserted_PATH = KML_inserted_PATH_50m\n",
    "            print('-------------------------------------')\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対して、SQLサーバーにデータを挿入しますฅ^•ω•^ฅ')\n",
    "            print('')\n",
    "            print('50m間隔のsegmentデータが挿入中です')\n",
    "            print('')\n",
    "            print('今挿入中のテーブルはSEGMENTS_****_50M ')\n",
    "            print('')\n",
    "            insert_data_to_SEGMENTS_****_50M(Data_for_inserting_SEGMENTS_****)\n",
    "            print('SEGMENTS_****_50M挿入完了')\n",
    "            print('')\n",
    "            print('今挿入中のテーブルはSEMANTIC_LINKS_SEGMENTS_50M')\n",
    "            print('')\n",
    "            insert_data_to_SEMANTIC_LINKS_SEGMENTS_50M(Data_for_inserting_SEMANTIC_LINKS_SEGMENTS)\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対して、','全て挿入完了')\n",
    "            print('-------------------------------------')\n",
    "        elif distance_between_segments == 100:\n",
    "            KML_inserted_PATH = KML_inserted_PATH_100m\n",
    "            print('-------------------------------------')\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対して、SQLサーバーにデータを挿入しますฅ^•ω•^ฅ')\n",
    "            print('')\n",
    "            print('100m間隔のsegmentデータが挿入中です')\n",
    "            print('')\n",
    "            print('今挿入中のテーブルはSEGMENTS_****_100M')\n",
    "            print('')\n",
    "            insert_data_to_SEGMENTS_****_100M(Data_for_inserting_SEGMENTS_****)\n",
    "            print('SEGMENTS_****_100M挿入完了')\n",
    "            print('')\n",
    "            print('今挿入中のテーブルはSEMANTIC_LINKS_SEGMENTS_100M')\n",
    "            print('')\n",
    "            insert_data_to_SEMANTIC_LINKS_SEGMENTS_100M(Data_for_inserting_SEMANTIC_LINKS_SEGMENTS)\n",
    "            print('セマンティック',SEMANTIC_LINK_ID,'に対して、','全て挿入完了')\n",
    "            print('-------------------------------------')\n",
    "        else:\n",
    "            print('-------------------------------------')\n",
    "            print('挿入に失敗しました')\n",
    "            print('セグメント間隔の指定に問題があります、必ず範囲内に指定してください')\n",
    "            print('-------------------------------------')\n",
    "            break\n",
    "        #insert済みのものを移動する\n",
    "        move_file(filename)\n",
    "        \n",
    "        #生成したセグメントをレビューする\n",
    "        review_path = Path(rf\"{os.getcwd()}\\review_of_segment\\{SEMANTIC_LINK_ID}_{distance_between_segments}m.html\")\n",
    "        display_folium_map_from_csv(df_segment,review_path)\n",
    "    print('')\n",
    "    print('ฅ^•ω•^ฅすべてのKMLが挿入完了ニャー')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d903b",
   "metadata": {},
   "source": [
    "# Credit\n",
    "- ShichiryにkmlとJsonを処理する関数の部分のcodeを教えていただきました\n",
    "- 何新に欠損を埋める部分に協力していただきました\n",
    "- Ishigeにたくさんの助言をいただきました\n",
    "- Sogaにエネルギーマップについて親切かつ詳しく解説していただきました\n",
    "- prof.Tommyにsegmentと欠損について大変有用な助言をいただきました"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
